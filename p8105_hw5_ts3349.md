p8105\_hw5\_ts3349
================
Tessa Senders
11/9/2020

``` r
library(tidyverse)
```

    ## -- Attaching packages --------------------------------------- tidyverse 1.3.0 --

    ## v ggplot2 3.3.2     v purrr   0.3.4
    ## v tibble  3.0.3     v dplyr   1.0.2
    ## v tidyr   1.1.2     v stringr 1.4.0
    ## v readr   1.3.1     v forcats 0.5.0

    ## -- Conflicts ------------------------------------------ tidyverse_conflicts() --
    ## x dplyr::filter() masks stats::filter()
    ## x dplyr::lag()    masks stats::lag()

``` r
library(purrr)
library(stringr)
```

## Problem 1

``` r
homicide_df =
  read_csv("homicide_data/homicide-data.csv") %>%
  mutate(
    city_state = str_c(city, state, sep = "_"),
    resolved = case_when(
    disposition == "Closed without arrest" ~ "unsolved",
    disposition == "Open/No arrest" ~ "unsolved",
    disposition == "Closed by arrest" ~ "solved",
  )
) %>%
  select(city_state, resolved) %>%
  filter(city_state != "Tulsa_AL")
```

    ## Parsed with column specification:
    ## cols(
    ##   uid = col_character(),
    ##   reported_date = col_double(),
    ##   victim_last = col_character(),
    ##   victim_first = col_character(),
    ##   victim_race = col_character(),
    ##   victim_age = col_character(),
    ##   victim_sex = col_character(),
    ##   city = col_character(),
    ##   state = col_character(),
    ##   lat = col_double(),
    ##   lon = col_double(),
    ##   disposition = col_character()
    ## )

The raw data regarding homicides in all 50 states in the USA comes from
the Washington Post GitHub repository. The data includes, the victimâ€™s
name, race, and age. The data also includes the city, the state, the
latitude, and the longitude of the homicide and the disposition (whether
the homicide is Closed by arrest, Open/No arrest, or Closed without
arrest). The data has 52178 entries.

``` r
aggregate_df = homicide_df %>%
  group_by(city_state) %>%
  summarize(
    hom_total = n(),
    hom_unsolved = sum(resolved == "unsolved")
  ) 
```

    ## `summarise()` ungrouping output (override with `.groups` argument)

``` r
prop.test(
  aggregate_df %>% filter(city_state == "Baltimore_MD") %>% pull(hom_unsolved),
  aggregate_df %>% filter(city_state == "Baltimore_MD") %>% pull(hom_total)) %>%
  broom::tidy()
```

    ## # A tibble: 1 x 8
    ##   estimate statistic  p.value parameter conf.low conf.high method    alternative
    ##      <dbl>     <dbl>    <dbl>     <int>    <dbl>     <dbl> <chr>     <chr>      
    ## 1    0.646      239. 6.46e-54         1    0.628     0.663 1-sample~ two.sided

``` r
results_df = aggregate_df %>%
  mutate(
    prop_tests = map2(.x = hom_unsolved, .y = hom_total, ~prop.test(x = .x, n = .y)),
    tidy_tests = map(.x = prop_tests, ~broom::tidy(.x))
  ) %>%
  select(-prop_tests) %>%
  unnest(tidy_tests) %>%
  select(city_state, estimate, conf.low, conf.high) 

head(results_df)
```

    ## # A tibble: 6 x 4
    ##   city_state     estimate conf.low conf.high
    ##   <chr>             <dbl>    <dbl>     <dbl>
    ## 1 Albuquerque_NM    0.386    0.337     0.438
    ## 2 Atlanta_GA        0.383    0.353     0.415
    ## 3 Baltimore_MD      0.646    0.628     0.663
    ## 4 Baton Rouge_LA    0.462    0.414     0.511
    ## 5 Birmingham_AL     0.434    0.399     0.469
    ## 6 Boston_MA         0.505    0.465     0.545

``` r
results_df %>%
  mutate(city_state = fct_reorder(city_state, estimate)) %>%
  ggplot(aes(x = city_state, y = estimate)) +
  geom_point() +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) +
  theme(axis.text = element_text(angle = 270, vjust = 0.5, hjust = 1)) +
  labs(
    title = "Confidence Intervals for The Proportion of \n Unsolved Homicides in Various Cities",
    x = "City and State",
    y = "Proportion Estimate",
    caption = "Data from the Washington Post GitHub")
```

![](p8105_hw5_ts3349_files/figure-gfm/prob%201%20part%205-1.png)<!-- -->

## Problem 2

``` r
path_df = 
tibble(
path = list.files("lda_data"))

longitudinal_study_df = path_df %>%
  mutate(
    path = str_c("lda_data/", path),
    data = map(.x = path, ~read_csv(.x))
) %>%
  unnest(data) %>%
  pivot_longer(
    week_1:week_8,
    names_to = "week",
    values_to = "observation"
  ) %>%
  mutate(
    week = as.numeric(str_replace(week, "week_", "")),
    arm = str_extract(path, "/[a-z][a-z][a-z]"),
    arm = str_remove(arm, "/"),
    id = str_extract(path, "[0-9]+")
  ) %>%
  select(-path)
```

    ## Parsed with column specification:
    ## cols(
    ##   week_1 = col_double(),
    ##   week_2 = col_double(),
    ##   week_3 = col_double(),
    ##   week_4 = col_double(),
    ##   week_5 = col_double(),
    ##   week_6 = col_double(),
    ##   week_7 = col_double(),
    ##   week_8 = col_double()
    ## )
    ## Parsed with column specification:
    ## cols(
    ##   week_1 = col_double(),
    ##   week_2 = col_double(),
    ##   week_3 = col_double(),
    ##   week_4 = col_double(),
    ##   week_5 = col_double(),
    ##   week_6 = col_double(),
    ##   week_7 = col_double(),
    ##   week_8 = col_double()
    ## )
    ## Parsed with column specification:
    ## cols(
    ##   week_1 = col_double(),
    ##   week_2 = col_double(),
    ##   week_3 = col_double(),
    ##   week_4 = col_double(),
    ##   week_5 = col_double(),
    ##   week_6 = col_double(),
    ##   week_7 = col_double(),
    ##   week_8 = col_double()
    ## )
    ## Parsed with column specification:
    ## cols(
    ##   week_1 = col_double(),
    ##   week_2 = col_double(),
    ##   week_3 = col_double(),
    ##   week_4 = col_double(),
    ##   week_5 = col_double(),
    ##   week_6 = col_double(),
    ##   week_7 = col_double(),
    ##   week_8 = col_double()
    ## )
    ## Parsed with column specification:
    ## cols(
    ##   week_1 = col_double(),
    ##   week_2 = col_double(),
    ##   week_3 = col_double(),
    ##   week_4 = col_double(),
    ##   week_5 = col_double(),
    ##   week_6 = col_double(),
    ##   week_7 = col_double(),
    ##   week_8 = col_double()
    ## )
    ## Parsed with column specification:
    ## cols(
    ##   week_1 = col_double(),
    ##   week_2 = col_double(),
    ##   week_3 = col_double(),
    ##   week_4 = col_double(),
    ##   week_5 = col_double(),
    ##   week_6 = col_double(),
    ##   week_7 = col_double(),
    ##   week_8 = col_double()
    ## )
    ## Parsed with column specification:
    ## cols(
    ##   week_1 = col_double(),
    ##   week_2 = col_double(),
    ##   week_3 = col_double(),
    ##   week_4 = col_double(),
    ##   week_5 = col_double(),
    ##   week_6 = col_double(),
    ##   week_7 = col_double(),
    ##   week_8 = col_double()
    ## )
    ## Parsed with column specification:
    ## cols(
    ##   week_1 = col_double(),
    ##   week_2 = col_double(),
    ##   week_3 = col_double(),
    ##   week_4 = col_double(),
    ##   week_5 = col_double(),
    ##   week_6 = col_double(),
    ##   week_7 = col_double(),
    ##   week_8 = col_double()
    ## )
    ## Parsed with column specification:
    ## cols(
    ##   week_1 = col_double(),
    ##   week_2 = col_double(),
    ##   week_3 = col_double(),
    ##   week_4 = col_double(),
    ##   week_5 = col_double(),
    ##   week_6 = col_double(),
    ##   week_7 = col_double(),
    ##   week_8 = col_double()
    ## )
    ## Parsed with column specification:
    ## cols(
    ##   week_1 = col_double(),
    ##   week_2 = col_double(),
    ##   week_3 = col_double(),
    ##   week_4 = col_double(),
    ##   week_5 = col_double(),
    ##   week_6 = col_double(),
    ##   week_7 = col_double(),
    ##   week_8 = col_double()
    ## )
    ## Parsed with column specification:
    ## cols(
    ##   week_1 = col_double(),
    ##   week_2 = col_double(),
    ##   week_3 = col_double(),
    ##   week_4 = col_double(),
    ##   week_5 = col_double(),
    ##   week_6 = col_double(),
    ##   week_7 = col_double(),
    ##   week_8 = col_double()
    ## )
    ## Parsed with column specification:
    ## cols(
    ##   week_1 = col_double(),
    ##   week_2 = col_double(),
    ##   week_3 = col_double(),
    ##   week_4 = col_double(),
    ##   week_5 = col_double(),
    ##   week_6 = col_double(),
    ##   week_7 = col_double(),
    ##   week_8 = col_double()
    ## )
    ## Parsed with column specification:
    ## cols(
    ##   week_1 = col_double(),
    ##   week_2 = col_double(),
    ##   week_3 = col_double(),
    ##   week_4 = col_double(),
    ##   week_5 = col_double(),
    ##   week_6 = col_double(),
    ##   week_7 = col_double(),
    ##   week_8 = col_double()
    ## )
    ## Parsed with column specification:
    ## cols(
    ##   week_1 = col_double(),
    ##   week_2 = col_double(),
    ##   week_3 = col_double(),
    ##   week_4 = col_double(),
    ##   week_5 = col_double(),
    ##   week_6 = col_double(),
    ##   week_7 = col_double(),
    ##   week_8 = col_double()
    ## )
    ## Parsed with column specification:
    ## cols(
    ##   week_1 = col_double(),
    ##   week_2 = col_double(),
    ##   week_3 = col_double(),
    ##   week_4 = col_double(),
    ##   week_5 = col_double(),
    ##   week_6 = col_double(),
    ##   week_7 = col_double(),
    ##   week_8 = col_double()
    ## )
    ## Parsed with column specification:
    ## cols(
    ##   week_1 = col_double(),
    ##   week_2 = col_double(),
    ##   week_3 = col_double(),
    ##   week_4 = col_double(),
    ##   week_5 = col_double(),
    ##   week_6 = col_double(),
    ##   week_7 = col_double(),
    ##   week_8 = col_double()
    ## )
    ## Parsed with column specification:
    ## cols(
    ##   week_1 = col_double(),
    ##   week_2 = col_double(),
    ##   week_3 = col_double(),
    ##   week_4 = col_double(),
    ##   week_5 = col_double(),
    ##   week_6 = col_double(),
    ##   week_7 = col_double(),
    ##   week_8 = col_double()
    ## )
    ## Parsed with column specification:
    ## cols(
    ##   week_1 = col_double(),
    ##   week_2 = col_double(),
    ##   week_3 = col_double(),
    ##   week_4 = col_double(),
    ##   week_5 = col_double(),
    ##   week_6 = col_double(),
    ##   week_7 = col_double(),
    ##   week_8 = col_double()
    ## )
    ## Parsed with column specification:
    ## cols(
    ##   week_1 = col_double(),
    ##   week_2 = col_double(),
    ##   week_3 = col_double(),
    ##   week_4 = col_double(),
    ##   week_5 = col_double(),
    ##   week_6 = col_double(),
    ##   week_7 = col_double(),
    ##   week_8 = col_double()
    ## )
    ## Parsed with column specification:
    ## cols(
    ##   week_1 = col_double(),
    ##   week_2 = col_double(),
    ##   week_3 = col_double(),
    ##   week_4 = col_double(),
    ##   week_5 = col_double(),
    ##   week_6 = col_double(),
    ##   week_7 = col_double(),
    ##   week_8 = col_double()
    ## )

``` r
 longitudinal_study_df %>% ggplot(aes(x = week, y = observation, color = id, linetype = arm)) +
  geom_line() +
  labs(
    title = "Observations for Each Subject Over 8 Weeks",
    x = "Week Number",
    y = "Observation Value",
    caption = "Data from a longitudinal study that included \n a control arm and an experimental arm. \n  ID # 01-05 are the control arm and \n ID # 06-10 are the experimental arm.")
```

![](p8105_hw5_ts3349_files/figure-gfm/prob%202%20plot-1.png)<!-- -->

The plot shows that the subjects from the experimental arm (ids 01-05)
tend to have higher observed values over time (increasing values over
time) than the subjects from the control arm (ids 06-10). The observed
values for the subjects in the control arm tend to stay fairly constant
over time.

## Problem 3

``` r
set.seed(1)

sim_datasets = function(mu) {
  
    x = rnorm(n = 30, mean = mu, sd = 5)
  
  t.test(x = x, mu = 0, alternative = "two.sided") %>% 
    broom::tidy() %>% select(estimate, p.value)
}


sim_results = tibble(
  mu = c(0, 1, 2, 3, 4, 5, 6) 
  ) %>%
  mutate(output_lists = map(.x = mu, ~rerun(5000, sim_datasets(mu = .x)))
         ) %>%
  unnest(output_lists) %>%
  unnest(output_lists) %>%
  mutate(reject_null = if_else(p.value < 0.05, 1, 0))
```

``` r
sim_results %>% 
  group_by(mu) %>%
  summarize(prop_reject = sum(reject_null)/5000) %>%
  ggplot(aes(x = mu, y = prop_reject)) +
  geom_line() +
  labs(
    title = "The Proportion of Times the Null was Rejected for Each mu",
    x = "mu",
    y = "Proportion of Times the Null was Rejected",
    caption = "Data is the result of generating 5000 datasets \n from a normal distribution with a sample size of 30 \n and standard deviation of 5 for each of 7 different mu values (0-6).")
```

    ## `summarise()` ungrouping output (override with `.groups` argument)

![](p8105_hw5_ts3349_files/figure-gfm/prob%203%20plot%201-1.png)<!-- -->

As the effect size increases, the power increases. Since the standard
deviation is the same for the datasets for each mu, the only part of the
effect size that is changing is the difference between the actual mu and
the null mu. As the difference between the null mu (0) and the actual mu
increases, the power also increases.

``` r
avg_vs_true_plot = sim_results %>% 
  group_by(mu) %>%
  summarize(mean_mu_hat = mean(estimate)) %>%
  ggplot(aes(x = mu, y = mean_mu_hat)) +
  geom_line() +
  labs(
    title = "The Average Estimate of mu versus the True Value of mu",
    x = "True Value of mu",
    y = "Average Estimate of mu",
    caption = "Data is the result of generating 5000 datasets \n from a normal distribution with a sample size of 30 \n and standard deviation of 5 for each of 7 different mu values (0-6).")
```

    ## `summarise()` ungrouping output (override with `.groups` argument)

``` r
avg_rejected_vs_true_plot = sim_results %>% 
  filter(reject_null == 1) %>%
  group_by(mu) %>%
  summarize(mean_mu_hat = mean(estimate)) %>%
  ggplot(aes(x = mu, y = mean_mu_hat)) +
  geom_line() +
  labs(
    title = "The Average Estimate of mu \n (only for samples which the null was rejected) \n versus the True Value of mu",
    x = "True Value of mu",
    y = "Average Estimate of mu \n (only for samples which the null was rejected)",
    caption = "Data is the result of generating 5000 datasets \n from a normal distribution with a sample size of 30 \n and standard deviation of 5 for each of 7 different mu values (0-6).")
```

    ## `summarise()` ungrouping output (override with `.groups` argument)

``` r
avg_vs_true_plot
```

![](p8105_hw5_ts3349_files/figure-gfm/prob%203%20plots%20combined-1.png)<!-- -->

``` r
avg_rejected_vs_true_plot
```

![](p8105_hw5_ts3349_files/figure-gfm/prob%203%20plots%20combined-2.png)<!-- -->

The sample average mu across tests for which the null is rejected is
approximately equal to the true value of mu because when the null (mu =
0) is rejected that means the estimate is closer to the true mu (except
for when the true mu is 0).
