---
title: "p8105_hw5_ts3349"
author: "Tessa Senders"
date: "11/9/2020"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r load libraries}
library(tidyverse)
library(purrr)
library(stringr)
```


## Problem 1


```{r prob 1 part 1}
homicide_df =
  read_csv("homicide_data/homicide-data.csv") %>%
  mutate(
    city_state = str_c(city, state, sep = "_"),
    resolved = case_when(
    disposition == "Closed without arrest" ~ "unsolved",
    disposition == "Open/No arrest" ~ "unsolved",
    disposition == "Closed by arrest" ~ "solved",
  )
) %>%
  select(city_state, resolved) %>%
  filter(city_state != "Tulsa_AL")
```

The raw data regarding homicides in all 50 states in the USA comes from the Washington Post GitHub repository.  The data includes, the victim's name, race, and age.  The data also includes the city, the state, the latitude, and the longitude of the homicide and the disposition (whether the homicide is Closed by arrest, Open/No arrest, or Closed without arrest).  The data has `r nrow(homicide_df)` entries.

```{r prob 1 part 2}
aggregate_df = homicide_df %>%
  group_by(city_state) %>%
  summarize(
    hom_total = n(),
    hom_unsolved = sum(resolved == "unsolved")
  ) 
```

```{r prob 1 part 3}
prop.test(
  aggregate_df %>% filter(city_state == "Baltimore_MD") %>% pull(hom_unsolved),
  aggregate_df %>% filter(city_state == "Baltimore_MD") %>% pull(hom_total)) %>%
  broom::tidy()
```

```{r prob 1 part 4}
results_df = aggregate_df %>%
  mutate(
    prop_tests = map2(.x = hom_unsolved, .y = hom_total, ~prop.test(x = .x, n = .y)),
    tidy_tests = map(.x = prop_tests, ~broom::tidy(.x))
  ) %>%
  select(-prop_tests) %>%
  unnest(tidy_tests) %>%
  select(city_state, estimate, conf.low, conf.high) 

results_df
```

```{r prob 1 part 5}
results_df %>%
  mutate(city_state = fct_reorder(city_state, estimate)) %>%
  ggplot(aes(x = city_state, y = estimate)) +
  geom_point() +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) +
  theme(axis.text = element_text(angle = 270, vjust = 0.5, hjust = 1)) +
  labs(
    title = "Confidence Intervals for The Proportion of \n Unsolved Homicides in Various Cities",
    x = "City and State",
    y = "Proportion Estimate",
    caption = "Data from the Washington Post GitHub")
```


## Problem 2

Suggestions on how to solve from Jeff:
Keep homicide data in one folder and longitudianl date in another folder.

20 different csvs
control: week 1-week 8


Import one dataset:

read_csv across all 20 paths
have each stored in a df next to its path
csv path, tibble from csv

then mutate to see if people in control or not

weeks in a column and observations next to them (need to pivot!)

column (control or experimental), column (which week), column (what is observation value)-GOAL FOR FINAL TABLE

```{r prob 2 help from jeff}
data_1 = read_csv("lda_data/con_01.csv")

path_df = 
tibble(
path = list.files("lda_data") %>%
  mutate(path = strd_c("lda_data/", path)),
data = map(..........)
)

read.csv(path_df$path[[1]])
```


```{r prob 2 part 1}
path_df = 
tibble(
path = list.files("lda_data"))

path_df = path_df %>%
  mutate(
    path = str_c("lda_data/", path),
    data = map(.x = path, ~read_csv(.x))
) %>%
  unnest(data) %>%
  pivot_longer(
    week_1:week_8,
    names_to = "week",
    values_to = "observation"
  ) %>%
  mutate(
    week = as.numeric(str_replace(week, "week_", "")),
    arm = str_extract(path, "/[a-z][a-z][a-z]"),
    arm = str_remove(arm, "/"),
    id = str_extract(path, "[0-9]+")
  ) %>%
  select(-path)

```

Make a spaghetti plot showing observations on each subject over time, and comment on differences between groups

```{r prob 2 plot}
 path_df %>% ggplot(aes(x = week, y = observation, color = id)) +
  geom_line() +
  #theme(axis.text = element_text(angle = 270, vjust = 0.5, hjust = 1)) +
  labs(
    title = "Observations for Each Subject Over 8 Weeks",
    x = "Week Number",
    y = "Observation Value",
    caption = "Data from a longitudinal study that included \n a control arm and an experimental arm. \n  ID # 01-05 are the control arm and \n ID # 06-10 are the experimental arm.")
```



## Problem 3

simulate a sampel from a normal distribution-sample size of 30 and variance of 5 but mean will keep changing

for each mean-will generate 5000 datasets of sample size 30

do not compute mean and sd

exporting the results of a t test (is mean equal to 0?)

reject or fail to reject the null 5% of the time

for 6 hopefully larger percent

figure out how to use t.test

broom tidy to clean up t test results-get estimate and p value

making two plots-summaries of p values and estimates based on whether significant or not

estimated mean when true mean is 0

then filter for results where p <0.05 then compute the mean-what does that look like?

set.seed()

always testing null mu=0

```{r prob 3}
```

First set the following design elements:

Fix n=30
Fix σ=5
Set μ=0. Generate 5000 datasets from the model

x∼Normal[μ,σ]

For each dataset, save μ^ and the p-value arising from a test of H:μ=0 using α=0.05. Hint: to obtain the estimate and p-value, use broom::tidy to clean the output of t.test.


```{r prob 3 part 1}
sim_datasets = function(mu) {
  
    x = rnorm(n = 30, mean = mu, sd = 5)
  
  t.test(x = x, mu = 0, alternative = "two.sided") %>% 
    broom::tidy() %>% select(estimate, p.value)
  
  
           
}


sim_results = tibble(
  mu = c(0, 1, 2, 3, 4, 5, 6) 
  ) %>%
  mutate(output_lists = map(.x = mu, ~rerun(5000, sim_datasets(mu = .x)))
         ) %>%
  unnest(output_lists) %>%
  unnest(output_lists)

#View(sim_results)
```







